{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db32a678-9bb9-4d53-be87-a2a7192a11c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End to End ML with GPT-3.5\n",
    "Learn how to use GPT-3.5 to do the heavy lifting for data acquisition, preprocessing, model training, and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c6009-1158-4537-9600-e5ecc9477a2d",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a043891a-f069-490b-a14c-968567a3bb34",
   "metadata": {},
   "source": [
    "Code examples for extracting data, preprocessing, model training, and deployment is readily available on the internet, though gathering it, and integrating it into a project takes time. Since such code is on the internet, chances are it has been trained on by a large language model (LLM) and can be rearranged in a variety of useful ways through natural language commands. The goal of this notebook is to show how easy it is to automate many of the steps common to ML projects by using the GPT-3.5 API from OpenAI. We’ll show some failure cases along the way, and how to tune prompts to fix bugs when possible. Starting from scratch, without even so much as a dataset, we’ll end up with a model that is ready to be deployed on AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e335bc4-23cc-413b-ac9c-f9d778039f90",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a726d04-964f-4880-b38d-07778b3cdfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] #os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def get_api_result(prompt):\n",
    "    request = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0301\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    result = request['choices'][0]['message']['content']\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874e057-c290-43e0-ace6-d95b02e66d81",
   "metadata": {},
   "source": [
    "## Extract, transform, load (ETL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b7a0cd6-039b-443c-a477-0bf616033302",
   "metadata": {},
   "source": [
    "This section is simplified since it only considers a single data source, but can in principle be extended so situations where data comes from multiple sources (csv files, databases, images, etc.). The first step is to extract some data. For the sake of simplicity, Wק’ll use the Income Prediction dataset where the goal is to predict if an individual earns more/less than $50k per year based on their education, job position, industry, etc. The function below will be used to generate the code that downloads our data. Note how the prompt template is designed to bias the API to generate python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6013a881-013a-47c0-812d-3ce8904b5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate Python code. \n",
    "    Please provide a natural language input text, and I will generate the corresponding Python code.\n",
    "    \\nInput: {prompt}\n",
    "    \\nPython code:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66c76af-21fd-4879-a152-f54583802e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sklearn.datasets as datasets\n",
      "\n",
      "# Fetch the adult income prediction dataset from openml using fetch_openml function\n",
      "adult_income_dataset = datasets.fetch_openml(name='adult', version=2)\n",
      "\n",
      "# Load the dataset into a pandas dataframe with target in a column named 'target'\n",
      "df = pd.DataFrame(adult_income_dataset.data, columns=adult_income_dataset.feature_names)\n",
      "df['target'] = adult_income_dataset.target\n",
      "-------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = '''\n",
    "Retrieve the adult income prediction dataset from openml using the sklearn fetch_openml function. \n",
    "Make sure to retrieve the data as a single dataframe \"df\" which includes the target in a column named “target”. \n",
    "'''\n",
    "\n",
    "extract(prompt)\n",
    "print('-------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db61d145-cb46-4325-8399-e97a6e56b00a",
   "metadata": {},
   "source": [
    "This code is free of bugs and gives us exactly what we want. \n",
    "Had we used a simpler prompt by removing mentions of openml and the function to use for retrieval, we would get the follwoing result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf38435-8f35-4342-af5f-de2ec77e2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('adult_income_prediction.csv')\n",
      "df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'target']\n",
      "-------------\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = '''\n",
    "Retrieve the adult income prediction dataset from csv.\n",
    "Make sure to retrieve the data as a single dataframe which includes the target in a column named “target”. \n",
    "Asign names to other columns too.\n",
    "'''\n",
    "\n",
    "extract(prompt)\n",
    "print('-------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccab2f0c-ae73-4011-8194-bcfdd1436340",
   "metadata": {},
   "source": [
    "This assumes that the data is locally available. What’s interesting about this result is that it has the correct column names as a list, even though we did not include them in the API call! These names nevertheless are all over the web, particularly in this Medium post, except for the target column which is added by GPT. The next step is to transform the data into a format that is usable by machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c5f0e70-5ca2-4a0e-b259-6e5fb0dac857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(columns, column_types, prompt):\n",
    "def transform(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate Python code. \n",
    "    Please provide a natural language input text, \n",
    "    and I will generate the corresponding Python code using the Pandas to preprocess the given DataFrame df. \n",
    "    \\nInput: {prompt}\n",
    "    \\nPython code:\"\"\"\n",
    "    # The DataFrame columns are {columns} and their corresponding dtypes are {column_types}.\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09e3638-714d-4a0b-a51e-c9adb14987bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Read in the dataframe\n",
      "df = pd.read_csv('your_data.csv')\n",
      "\n",
      "# Convert categorical columns to one-hot encoding\n",
      "df = pd.get_dummies(df)\n",
      "\n",
      "# Normalize numerical columns\n",
      "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
      "scaler = StandardScaler()\n",
      "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
      "\n",
      "# Drop rows with NA or NaN values\n",
      "df.dropna(inplace=True)\n",
      "\n",
      "# Drop rows with numeric column outliers as determined by their z score\n",
      "z_scores = pd.DataFrame()\n",
      "for col in num_cols:\n",
      "    z_scores[col] = (df[col] - df[col].mean()) / df[col].std(ddof=0)\n",
      "    z_scores = z_scores.abs().lt(3)\n",
      "df = df[z_scores.all(axis=1)]\n",
      "\n",
      "# Convert target column values to 0 or 1 and change column type to int\n",
      "df['target_column'] = df['target_column'].apply(lambda x: 0 if x == 'value_1' else 1).astype(int)\n",
      "-----------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 8.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = '''\n",
    "Preprocess the dataframe by converting all categorical columns to their one-hot encoded equivalents, and normalizing numerical columns. \n",
    "Drop rows which have an NA or NaN value in any column. Drop rows that have numeric column outliers as determined by their z score. \n",
    "A numeric column outlier is a value that is outside of the 1 to 99 inter-quantile range. \n",
    "The numerical columns should be normalized using StandardScaler from sklearn. \n",
    "The values in the target colummn should be converted to 0 or 1 and should be of type int.\n",
    "'''\n",
    "transform(prompt)\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac83e6e8-21bc-4c78-8d8c-475f7352e9b4",
   "metadata": {},
   "source": [
    "Lastly, we need to load the data into a local database. \n",
    "This is overkill for such a simple use case, but is a good habit to develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96578763-d909-43fe-befc-da6c4dc7d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate Python code. \n",
    "    Please provide a natural language input text, and I will generate the corresponding Python code.\n",
    "    \\nInput: {prompt}\n",
    "    \\nPython code:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b05cf5e0-f902-4ca7-8a55-e8af8bc27c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect to an SQLite database named \"data\", you can use the following code:\n",
      "\n",
      "```python\n",
      "import sqlite3\n",
      "\n",
      "conn = sqlite3.connect(\"data.db\")\n",
      "```\n",
      "\n",
      "Next, to insert data from a pandas DataFrame named \"df\" into a table named \"income\" without including the index column, you can use the following code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "df.to_sql(\"income\", conn, if_exists=\"replace\", index=False)\n",
      "```\n",
      "\n",
      "Here, `df` is the pandas DataFrame that you want to insert, `if_exists` is set to \"replace\" to ensure that any existing table with the same name is dropped and recreated, and `index` is set to `False` to exclude the index column from the table.\n",
      "\n",
      "Finally, you can commit the changes and close the connection using the following code:\n",
      "\n",
      "```python\n",
      "conn.commit()\n",
      "conn.close()\n",
      "```\n",
      "-----------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = '''Connect to an sqlite database named “data”. \n",
    "Use pandas to insert data from a DataFrame named “df” into a table named “income”. \n",
    "Do not include the index column. \n",
    "Commit the changes before closing the connection.'''\n",
    "\n",
    "load(prompt)\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b048b9-7ea5-486f-a885-f523d720c5f8",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "696483f4-98d8-4c2a-880b-a2b0a6136750",
   "metadata": {},
   "source": [
    "Resources permitting, it is a good idea to try out a few different model types to identify the one with the right level of complexity for the given task. \n",
    "Therefore, we ask GPT-3.5 to try out a few different models. \n",
    "First, let’s set up the generic prompt template for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78cd7cc3-1a3d-450c-b5d6-4e9d63a74a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate Python code. \n",
    "    Focus on using scikit-learn when applicable. \n",
    "    Please provide a natural language input text, and I will generate the corresponding Python code.\n",
    "    \\nInput: {prompt}\n",
    "    \\nPython code:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d5499a7-d5e6-4c0d-8013-792da313606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
      "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import mlflow\n",
      "from mlflow.sklearn import log_model\n",
      "\n",
      "# Load dataframe\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Define X and y\n",
      "X = df.drop('target', axis=1)\n",
      "y = df['target']\n",
      "\n",
      "# Define classification models\n",
      "models = [RandomForestClassifier(), GradientBoostingClassifier(), LogisticRegression()]\n",
      "\n",
      "# Define hyperparameters to tune for each model\n",
      "params = [{'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15]},\n",
      "          {'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15], 'learning_rate': [0.01, 0.1]},\n",
      "          {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}]\n",
      "\n",
      "# Define scoring metrics to log\n",
      "scoring = ['accuracy', 'roc_auc', 'f1']\n",
      "\n",
      "# Start mlflow run\n",
      "mlflow.start_run()\n",
      "\n",
      "# Define cross-validation\n",
      "cv = 5\n",
      "\n",
      "# Loop over all models\n",
      "for i, model in enumerate(models):\n",
      "  \n",
      "    # Perform grid search to tune hyperparameters\n",
      "    clf = GridSearchCV(model, params[i], cv=cv)\n",
      "    clf.fit(X, y)\n",
      "    best_params = clf.best_params_\n",
      "\n",
      "    # Use best hyperparameters to train model and obtain metrics\n",
      "    clf_best = clf.best_estimator_\n",
      "    scores = cross_val_score(clf_best, X, y, cv=cv, scoring=scoring)\n",
      "\n",
      "    # Log metrics\n",
      "    for j, metric in enumerate(scoring):\n",
      "        metric_avg = round(scores[:, j].mean(), 4)\n",
      "        mlflow.log_metric(f'{metric}_{i}', metric_avg)\n",
      "\n",
      "    # Log best model\n",
      "    if i == 0:\n",
      "        best_model = clf_best\n",
      "    else:\n",
      "        if scores[:, 0].mean() > cross_val_score(best_model, X, y, cv=cv, scoring='accuracy').mean():\n",
      "            best_model = clf_best\n",
      "\n",
      "# Log best model\n",
      "log_model(best_model, 'best_model')\n",
      "\n",
      "# End mlflow run\n",
      "mlflow.end_run()\n",
      "-----------------------\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = '''\n",
    "Train a variety of classification models to predict the “target” column using all other columns. \n",
    "Do so using 5-fold cross validation to choose the best model and corresponding set of hyperparameters.\n",
    "Return the best overall model and corresponding hyperparameter settings. \n",
    "Choose the best model based on accuracy. \n",
    "Assume a dataframe named “df” exists which is to be used for training. \n",
    "Log the entire process using MLFlow. \n",
    "Start logging with mlflow before training any models so only a single run is stored. \n",
    "Make sure that the model is logged using the sklearn module of mlflow. \n",
    "Make sure that only the best overall model is logged, but log metrics for all model types. \n",
    "The mean value of the following metrics on all cross validation folds should be logged: accuracy, AUC, F1 score.\n",
    "'''\n",
    "\n",
    "train(prompt)\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f55b07b-dae0-4951-bbdd-5c3ea9037edd",
   "metadata": {},
   "source": [
    "If we remove the loading of df and the section # set up mlflow , we end up with exactly what is desired. Namely, a loop over a 3 different model types, performing a grid search using 5-fold cross validation to identify the best hyperparmeters for the given model type, while keeping track of metrics. Without specifying “choose the best model based on accuracy”, the generated code will use scoring=[“accuracy”, “roc_auc\", “f1”] for the grid search which will not work since there is ambiguity as to how to select the best model according to multiple metrics. Without “make sure that the model is logged using the sklearn module of mlflow”, we sometimes end up with mlflow.log_model() which is wrong. Also, “make sure that only the best overall model is logged” is necessary to avoid storing all models. Overall, this output is acceptable, but it’s unstable, and running it multiple times is likely to introduce different bugs. In order to have everything ready for the serving step, it is useful to add the model signature when saving the model. This signature is basically a collection of feature names and their corresponding types. \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6239da30-5812-4e0c-92b6-f982ad69ce74",
   "metadata": {},
   "source": [
    "It is a pain to get GPT-3.5 to add this, so some manual labor has to be done by first adding the import:\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "and then modifying the line of code which logs the model via:\n",
    "\n",
    "mlflow.sklearn.log_model(sk_model=best_model, artifact_path=\"best_model\", signature=infer_signature(df[features], best_model.predict(df[features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544dad2-53a8-442c-a229-006596f02e95",
   "metadata": {},
   "source": [
    "## Model Serving"
   ]
  },
  {
   "cell_type": "raw",
   "id": "543e77a9-00ca-4860-9c97-4edb2bc5e804",
   "metadata": {},
   "source": [
    "Since we used MLflow to log the best model, we have a couple of options to serve the model. \n",
    "The simplest option is to host the model locally. Let’s first design the general prompt template for model serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1134dace-b3a3-4513-9091-7b1a610d55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_model(model_path, prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate shell code for deploying models using MLFlow. \n",
    "    Please provide a natural language input text, and I will generate the corresponding command to deploy the model. \n",
    "    The model is located in the file {model_path}.\n",
    "    \\nInput: {prompt}\n",
    "    \\nShell command:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0025545-76c3-465d-9f3d-ad421960fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow models serve -m <model path here> -p 1111 --no-conda\n",
      "-------------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = '''Serve the model using port number 1111, and use the local environment manager'''\n",
    "model_path = '<model path here>'\n",
    "\n",
    "serve_model(model_path, prompt)\n",
    "\n",
    "print('-------------------------')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ed1a07b-eca7-4340-88fe-f8a587682016",
   "metadata": {},
   "source": [
    "Once we run this command in the shell, we are ready to make predictions by sending data encoded as JSON to the model. \n",
    "We’ll first generate the command to send data to the model, and then create the JSON payload to be inserted into the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49154290-4ef1-4259-9781-3842fe4c4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate code for sending data to deployed MLFlow models. \n",
    "    Please provide a natural language input text, and I will generate the corresponding command. \n",
    "    \\nInput: {prompt}\n",
    "    \\nCommand:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e07964b-b35b-44e4-a2ef-fd2749b5cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl -X POST -H \"Content-Type: application/json\" -d '{\"data here\"}' http://localhost:1111/invocations\n",
      "-------------------------\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = '''Use the “curl” command to send data “<data here>” to an mlflow model hosted at port 1111 on localhost. \n",
    "Make sure that the content type is \"application/json\".\n",
    "Write the command in one line'''\n",
    "\n",
    "send_request(prompt)\n",
    "print('-------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faf5ca2f-e608-4f89-b1bc-e66da657243f",
   "metadata": {},
   "source": [
    "It is preferable to have the URL immediately after curl instead of being at the very end of the command, i.e.\n",
    "\n",
    "curl http://localhost:1111/invocations -X POST -H \"Content-Type: application/json\" -d '<data here>'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f881753c-6f7b-44ae-8270-bd13e1e35a5f",
   "metadata": {},
   "source": [
    "Getting GPT-3.5 to do this is not easy. Both of the following requests fail to do so:\n",
    "\n",
    "Use the “curl” command to send data “<data here>” to an mlflow model hosted at port 1111 on localhost. Place the URL immediately after “curl”. Make sure that the content type is “application/json”.\n",
    "\n",
    "Use the “curl” command, with the URL placed before any argument, to send data “<data here>” to an mlflow model hosted at port 1111 on localhost. Make sure that the content type is “application/json”."
   ]
  },
  {
   "cell_type": "raw",
   "id": "293a8cde-55eb-4fb7-b438-f466da09909a",
   "metadata": {},
   "source": [
    "Maybe it’s possible to get the desired output if we have GPT-3.5 modify an existing command rather than generate one from scratch. \n",
    "Here is the generic template for modifying commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ce96730-d4f7-44d8-b4ae-ff2626b8c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_request(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can modify commands for sending data using \"curl\". \n",
    "    Please provide a natural language instruction, corresponding command, and I will generate the modified command. \n",
    "    \\nInput: {prompt}\n",
    "    \\nCommand:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ca0d505-917e-4b47-a9f2-d2f918830c4d",
   "metadata": {},
   "source": [
    "We will call this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a46684ed-7573-4a07-88e0-08fd233a299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Add the url http://example.com before the -X POST argument\"\n",
      "\n",
      "Modified command: curl http://example.com -X POST -H \"Content-Type: application/json\" -d '<data here>' http://localhost:1111/invocations\n",
      "-------------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "code = \"\"\"curl -X POST -H \"Content-Type: application/json\" -d '<data here>' http://localhost:1111/invocations\"\"\"\n",
    "prompt = f\"\"\"Please modify the following by placing the url before the \"-X POST\" argument:\\n{code}\"\"\"\n",
    "\n",
    "modify_request(prompt)\n",
    "print('-------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81c65448-c148-4662-bf29-976b2dd86663",
   "metadata": {},
   "source": [
    "Now time to create the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "446ff1ff-a47a-4702-b7dc-6036f901d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(prompt):\n",
    "    prompt_template = f\"\"\"You are a ChatGPT language model that can generate code for sending data to deployed MLFlow models. \n",
    "    Please provide a natural language input text, and I will generate the corresponding command. \n",
    "    \\nInput: {prompt}\n",
    "    \\nPython code:\"\"\"\n",
    "\n",
    "    get_api_result(prompt_template)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1422a91-5e3c-4611-918d-7c9485ea78a5",
   "metadata": {},
   "source": [
    "The prompt for this part needed quite a bit of tuning to get the desired output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05ff3992-3280-4c4f-9fda-29f13fabacfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "df = pd.DataFrame() # Replace with actual data\n",
      "\n",
      "# Drop the target column\n",
      "df = df.drop(['target'], axis=1)\n",
      "\n",
      "# Convert DataFrame to JSON format\n",
      "json_data = df.to_json(orient='split')\n",
      "\n",
      "# Wrap JSON data in an object and rename it\n",
      "new_json_data = {\"dataframe_split\": json.loads(json_data)}\n",
      "\n",
      "# Replace all single quotes with double quotes\n",
      "json_string = json.dumps(new_json_data).replace(\"'\", '\"')\n",
      "\n",
      "# Finally, send the JSON-formatted data to the deployed MLFlow model\n",
      "# with the appropriate API call.\n",
      "-------------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = '''\n",
    "Convert the DataFrame “df” to json format that can be received by a deployed MLFlow model. \n",
    "Wrap the resulting json in an object called “dataframe_split”. \n",
    "Also, “dataframe_split” should be surrounded by doubles quotes instead of single quotes. \n",
    "Do not include the “target” column. \n",
    "Use the split “orient” argument\n",
    "'''\n",
    "# The resulting string should not have newlines, and it should not escape quotes. \n",
    "\n",
    "create_payload(prompt)\n",
    "print('-------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "731840aa-4d9f-4392-a6d1-34be7d88f650",
   "metadata": {},
   "source": [
    "Without the explicit instruction to avoid newlines and escaping quotes, a call to json.dumps() is made which is not the format that the MLflow endpoint expects.\n",
    "\n",
    "Before replacing <data here> in the curl request with the value of wrapped_data, we probably want to send only a few rows of data for prediction, \n",
    "otherwise the resulting payload is too large. \n",
    "So we modify the above to be:\n",
    "\n",
    "json_data = df[:5].drop(\"target\", axis=1).to_json(orient=\"split\", double_precision=15)\n",
    "wrapped_data = f'{{\"dataframe_split\":{json_data}}}'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c993d743-8422-4228-ae68-38665c9116ee",
   "metadata": {},
   "source": [
    "Invoking the model gives:\n",
    "\n",
    "{\"predictions\": [0, 0, 0, 1, 0]}\n",
    "\n",
    "whereas the actual targets are [0, 0, 1, 1, 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2c9f4-72ae-408c-8922-04a0853a3dca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ad71aaf-d13a-436b-80ef-c40471ccf209",
   "metadata": {},
   "source": [
    "There we have it. At the beginning of this post, we didn’t even have access to a dataset, yet we’ve managed to end up with a deployed model that was selected to be the best through cross-validation. Importantly, GPT-3.5 did all the heavy lifting, and only required minimal assistance along the way. I did however have to specify particular libraries to use and methods to call, but this was mainly required to resolve ambiguities. Had I specified “Log the entire process” instead of “Log the entire process using MLFlow”, GPT-3.5 would have too many libraries to choose from, and the resulting model format might not have been useful for serving with MLflow. Thus, some knowledge of the tools used to perform the various steps in the ML pipeline is required to have success using GPT-3.5, but it is minimal compared to the knowledge required to code from scratch."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2092619b-fb96-4731-97eb-3b229091b66a",
   "metadata": {},
   "source": [
    "Another option for serving the model is to host it as a SageMaker endpoint on AWS. \n",
    "Despite how easy this may look on the MLflow website, I assure you that as with many examples on the web involving AWS, things will go wrong. \n",
    "First of all, Docker must be installed in order to generate the Docker Imager using the command:\n",
    "\n",
    "mlflow sagemaker build-and-push-container"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5a7e9ed-5c00-4b82-b126-795f42f72f15",
   "metadata": {},
   "source": [
    "Second, the Python library boto3 used to communicate with AWS also requires installation. \n",
    "Beyond this, permissions must be properly setup such that SageMaker, ECR, and S3 services can communicate with each other on behalf of your account. \n",
    "Here are the commands I ended up having to use:\n",
    "\n",
    "mlflow deployments run-local -t sagemaker -m <model path> --name income_classifier\n",
    "mlflow deployments create -t sagemaker --name income_classifier -m model/ --config image_url=<docker image url> --config bucket=mlflow-serving --config region_name=us-east-1\n",
    "\n",
    "along with some manual tinkering behind the scenes to get the S3 bucket to be in the correct region."
   ]
  },
  {
   "cell_type": "raw",
   "id": "008d5cf9-ff8d-44e5-8c8d-da4b777edaaa",
   "metadata": {},
   "source": [
    "With the help of GPT-3.5 we went through the ML pipeline in a (mostly) painless way, though the last mile was a bit trickier. Note how we didn’t use GPT-3.5 to generate the commands for serving the model on AWS. It works poorly for this use case, and creates made up argument names. I can only speculate that switching to the GPT-4.0 API would help resolve some of the above bugs, and lead to an even easier model development experience.\n",
    "\n",
    "While the ML pipeline can be fully automated using LLMs, it isn’t yet safe to have a non-expert be responsible for the process. The bugs in the above code were easily identified because the Python interpreter would throw errors, but there are more subtle bugs that can be harmful. For example, the elimination of outlier values in the preprocessing code could be wrong leading to excess or insufficient samples being discarded. In the worst case, it could inadvertently drop entire subgroups of people, exacerbating potential fairness issues.\n",
    "\n",
    "Additionally, the grid search over hyperparameters could have been done over a poorly chosen range, leading to overfitting or underfitting depending on the range. This would be quite tricky to identify for someone with little ML experience as the code otherwise seems correct, but an understanding of how regularization works in these models is required. Thus, it isn’t yet appropriate to have an unspecialized software engineer stand in for an ML engineer, but that time is fast approaching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
